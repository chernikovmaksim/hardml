{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb8c426-7422-4551-9ba7-8feb0c8ca5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from catboost.datasets import msrank_10k\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ListNet(torch.nn.Module):\n",
    "    def __init__(self, num_input_features: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # model architecture\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(num_input_features, self.hidden_dim),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_1: torch.Tensor) -> torch.Tensor:\n",
    "        logits = self.model(input_1)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class Solution:\n",
    "    \"\"\"\n",
    "    Class for training and testing ListNet model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_epochs: int = 5, \n",
    "        listnet_hidden_dim: int = 30, \n",
    "        lr: float = 0.001, \n",
    "        ndcg_top_k: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_epochs (int): Number of epochs.\n",
    "            listnet_hidden_dim (int): ListNet model hidden dim size.\n",
    "            lr (float): Learning rate.\n",
    "            ndcg_top_k (int): NDCG metric score for top k.\n",
    "        \"\"\"\n",
    "        self._prepare_data()\n",
    "        self.num_input_features = self.X_train.shape[1]\n",
    "        self.n_train = self.X_train.shape[0]\n",
    "        self.ndcg_top_k = ndcg_top_k\n",
    "        self.n_epochs = n_epochs\n",
    "\n",
    "        self.model = self._create_model(\n",
    "            self.num_input_features, listnet_hidden_dim\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "    def _get_data(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Method loads data for training and testing.\n",
    "        \"\"\"\n",
    "        train_df, test_df = msrank_10k()\n",
    "\n",
    "        X_train = train_df.drop([0, 1], axis=1).values\n",
    "        y_train = train_df[0].values\n",
    "        query_ids_train = train_df[1].values.astype(int)\n",
    "\n",
    "        X_test = test_df.drop([0, 1], axis=1).values\n",
    "        y_test = test_df[0].values\n",
    "        query_ids_test = test_df[1].values.astype(int)\n",
    "\n",
    "        return [X_train, y_train, query_ids_train, X_test, y_test, query_ids_test]\n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Method prepares data for training and test.\n",
    "        \"\"\"\n",
    "        (X_train, y_train, self.query_ids_train,\n",
    "            X_test, y_test, self.query_ids_test) = self._get_data()\n",
    "        X_train = self._scale_features_in_query_groups(X_train, self.query_ids_train)\n",
    "        X_test = self._scale_features_in_query_groups(X_test, self.query_ids_test)\n",
    "        self.X_train, self.ys_train, self.X_test, self.ys_test = map(torch.FloatTensor,\n",
    "                                                                    [X_train, y_train, X_test, y_test])\n",
    "        self.query_ids_train_u = np.unique(self.query_ids_train)\n",
    "        self.query_ids_test_u = np.unique(self.query_ids_test)\n",
    "\n",
    "    def _scale_features_in_query_groups(\n",
    "        self, \n",
    "        inp_feat_array: np.ndarray, \n",
    "        inp_query_ids: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Method normalizes the input data.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for q_id in np.unique(inp_query_ids):\n",
    "            result.append(StandardScaler().fit_transform(inp_feat_array[inp_query_ids == q_id]))\n",
    "        return np.vstack(result)  \n",
    "\n",
    "    def _create_model(\n",
    "        self, \n",
    "        listnet_num_input_features: int, \n",
    "        listnet_hidden_dim: int\n",
    "    ) -> torch.nn.Module:\n",
    "        \"\"\"\n",
    "        Method normalizes the input data.\n",
    "        \"\"\"\n",
    "        torch.manual_seed(0)\n",
    "        net = ListNet(listnet_num_input_features, listnet_hidden_dim)\n",
    "        return net\n",
    "    \n",
    "    def fit(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Method train and evaluates ListNet model on N epochs.\n",
    "        \n",
    "        Returns:\n",
    "            List[float]: NCDG metric score on epochs.\n",
    "        \"\"\"\n",
    "        val_metrics = []\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self._train_one_epoch(epoch)\n",
    "            val_metrics.append(self._eval_test_set())\n",
    "\n",
    "        return val_metrics\n",
    "\n",
    "    def _calc_loss(\n",
    "        self, \n",
    "        batch_ys: torch.FloatTensor,\n",
    "        batch_pred: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Method calculate Kullback-Leibler divergence loss.\n",
    "        https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "        \n",
    "        Args:\n",
    "            batch_ys (torch.FloatTensor): (n_i, 1) GT\n",
    "            batch_pred (torch.FloatTensor): (n_i, 1) preds\n",
    "            \n",
    "        Returns:\n",
    "            torch.FloatTensor: KL loss.\n",
    "        \"\"\"\n",
    "        P_y_i = torch.softmax(batch_ys, dim=0)\n",
    "        P_z_i = torch.softmax(batch_pred, dim=0)\n",
    "        return -torch.sum(P_y_i * torch.log(P_z_i/P_y_i))\n",
    "\n",
    "\n",
    "    def _train_one_epoch(self, epoch) -> None:\n",
    "        \"\"\"\n",
    "        Method train ListNet model on test train set.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        idx = torch.randperm(self.n_train)\n",
    "\n",
    "        X_train = self.X_train[idx]\n",
    "        y_train = self.ys_train[idx]\n",
    "        query_ids_train = self.query_ids_train[idx]\n",
    "\n",
    "        for query_id in self.query_ids_train_u:\n",
    "            idx_q = query_ids_train == query_id\n",
    "            X_q = X_train[idx_q]\n",
    "            y_q = y_train[idx_q]\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if len(y_q) > 0:\n",
    "                pred = self.model(X_q).flatten()\n",
    "                loss = self._calc_loss(y_q, pred)\n",
    "                loss.backward(retain_graph=True)\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def _eval_test_set(self) -> float:\n",
    "        \"\"\"\n",
    "        Method evaluates ListNet model on test set.\n",
    "        \n",
    "        Returns:\n",
    "            float: NCDG at k on test set.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            ndcgs = []\n",
    "            for query_id in self.query_ids_test_u:\n",
    "                idx_q = self.query_ids_test == query_id\n",
    "                X_q = self.X_test[idx_q]\n",
    "                y_q = self.ys_test[idx_q]\n",
    "                valid_pred_q = self.model(X_q).flatten()\n",
    "                ndcg = self._ndcg_k(y_q, valid_pred_q, \"exp2\", self.ndcg_top_k)\n",
    "                if ndcg > 1 or math.isnan(ndcg) or ndcg < 0:\n",
    "                    ndcg = 0.0\n",
    "                ndcgs.append(ndcg)\n",
    "                \n",
    "        return np.mean(ndcgs)\n",
    "\n",
    "    def _compute_gain(self, y_value: float, gain_scheme: str) -> float:\n",
    "        \"\"\"\n",
    "        Method for calculating DCG and NDCG, which calculates Gain.\n",
    "        \"\"\"\n",
    "        if gain_scheme == \"const\":\n",
    "            return y_value\n",
    "        elif gain_scheme == \"exp2\":\n",
    "            return 2**y_value - 1\n",
    "        else:\n",
    "            raise ValueError(f\"{gain_scheme} gain method not supported\")\n",
    "                          \n",
    "    def _dcg_k(\n",
    "        self, \n",
    "        ys_true: torch.Tensor, \n",
    "        ys_pred: torch.Tensor, \n",
    "        gain_scheme: str, \n",
    "        k: int = None\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the DCG at k. \n",
    "        https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "        \n",
    "        Args:\n",
    "            ys_true (torch.Tensor): Tensor of actual relevancy values.\n",
    "            ys_pred (torch.Tensor): Tensor of predicted relevancy values.\n",
    "            gain_scheme (str): Gain scheme.\n",
    "            k (int) : Choose highest k scores in the ranking.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: DCG at k metric.\n",
    "        \"\"\"\n",
    "        order = torch.argsort(ys_pred, descending=True, dim=-1)\n",
    "        true_sorted_by_preds = torch.gather(ys_true, dim=-1, index=order)\n",
    "\n",
    "        gain_function = lambda x: self._compute_gain(x, gain_scheme)\n",
    "        gains = gain_function(true_sorted_by_preds)\n",
    "\n",
    "        discounts = torch.tensor(1) /  torch.log2(torch.arange(true_sorted_by_preds.shape[0], dtype=torch.double) + 2.0)\n",
    "        if k is not None:\n",
    "            discounts[k:] = 0\n",
    "        discounted_gains = gains * discounts\n",
    "\n",
    "        sum_dcg = torch.sum(discounted_gains, dim=-1)\n",
    "        return float(sum_dcg)\n",
    "                          \n",
    "    def _ndcg_k(\n",
    "        self, \n",
    "        ys_true: torch.Tensor, \n",
    "        ys_pred: torch.Tensor, \n",
    "        gain_scheme: str = \"exp2\",\n",
    "        ndcg_top_k: int = None,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Method to calculate the NDCG at k. \n",
    "        https://en.wikipedia.org/wiki/Discounted_cumulative_gain\n",
    "        \n",
    "        Args:\n",
    "            ys_true (torch.Tensor): Tensor of actual relevancy values.\n",
    "            ys_pred (torch.Tensor): Tensor of predicted relevancy values.\n",
    "            gain_scheme (str): Gain scheme.\n",
    "            k (int) : Choose highest k scores in the ranking.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: NDCG at k metric.\n",
    "        \"\"\"\n",
    "        ideal_dcgs = self._dcg_k(ys_true, ys_true, gain_scheme, ndcg_top_k)\n",
    "        predicted_dcgs = self._dcg_k(ys_true, ys_pred, gain_scheme, ndcg_top_k)\n",
    "        ndcg_score = predicted_dcgs / ideal_dcgs\n",
    "        return ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c17d94-01fc-43a3-9f7b-9543a15f8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Solution(n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b6f500-41cc-435d-a34c-0ad6ee65d89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42946389355986125,\n",
       " 0.4369915362647493,\n",
       " 0.4388079099292035,\n",
       " 0.4426234048287631,\n",
       " 0.43943256481926923]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Solution(n_epochs=5)\n",
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c748d6-2771-4967-981a-37137769a365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
